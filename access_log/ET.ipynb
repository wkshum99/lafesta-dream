{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import string\n",
    "import numpy as np\n",
    "import helper as hp\n",
    "import sq\n",
    "import jp\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "Gb = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_csv = 'v:'+os.sep+'temp'+os.sep+'temp.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'v:'+os.sep+'temp'+os.sep+'log'+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_dir = 'v:'+os.sep+'temp'+os.sep+'a'+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitedb = 'v:\\\\temp\\\\access_log.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disk_engine = create_engine('sqlite:///'+sqlitedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store = pd.HDFStore('access_log.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#squid = sq.read_squid_log('access.log')\n",
    "#squid.tail()\n",
    "#safe_websites = 'google|microsoft|trendmicro|gstatic.com|bdpinsight.eu|.gov.hk'\n",
    "#access = squid[squid['URL'].str.contains(safe_websites) == False]\n",
    "#sum(squid.bytes)/Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def valid_file(file):\n",
    "    if (file[-4:] != '.zip' & file[0:9] == 'hk-ssg140'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2015-10-23 00:00:00\n",
      "1814554 rows processed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'type' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f7bddce2c9f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.zip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate 'str' and 'type' objects"
     ]
    }
   ],
   "source": [
    "print('Started at: ' + datetime.date.today().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "mode = 'single'\n",
    "if (mode == 'single'):\n",
    "    yesterday = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    log_file = os.path.join(log_dir, 'hk-ssg140.log-'+yesterday.strftime('%Y%m%d'))\n",
    "                                                                         \n",
    "    jp.clean_juniper_file(log_file, temp_csv)\n",
    "    \n",
    "    juniper = [] \n",
    "    temp_result = []\n",
    "    \n",
    "    juniper = jp.read_syslog_juniper(temp_csv)\n",
    "    \n",
    "    juniper['date'] = juniper['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    juniper['total_size'] = juniper['sent_size'] + juniper['received_size']        \n",
    "    \n",
    "    temp_result = jp.process_group(juniper)\n",
    "    \n",
    "    temp_result = pd.DataFrame(temp_result, columns=['date', 'src', 'dst', 'count', 'total_size'])\n",
    "    temp_result['location'] = 'HK'\n",
    "    \n",
    "    try:\n",
    "        temp_result.to_sql('data', disk_engine, index=False, if_exists='append')\n",
    "        print (log_file + ' processed')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    hp.zip_file(zip_dir + log_file + '.zip', os.path.join(log_dir, log_file))\n",
    "            \n",
    "    os.remove(os.path.join(log_dir, log_file))                                                                         \n",
    "                                                                         \n",
    "else:    \n",
    "    for file in os.listdir(log_dir):\n",
    "        if (valid_file(file)):\n",
    "            log_file = os.path.join(log_dir, file)\n",
    "            jp.clean_juniper_file(log_file, temp_csv)\n",
    "    \n",
    "            juniper = [] \n",
    "            temp_result = []\n",
    "    \n",
    "            juniper = jp.read_syslog_juniper(temp_csv)\n",
    "    \n",
    "            juniper['date'] = juniper['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            juniper['total_size'] = juniper['sent_size'] + juniper['received_size']        \n",
    "    \n",
    "            temp_result = jp.process_group(juniper)\n",
    "    \n",
    "            temp_result = pd.DataFrame(temp_result, columns=['date', 'src', 'dst', 'count', 'total_size'])\n",
    "            temp_result['location'] = 'HK'\n",
    "    \n",
    "            try:\n",
    "                temp_result.to_sql('data', disk_engine, index=False, if_exists='append')\n",
    "                print (file + ' processed')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            hp.zip_file(zip_dir + file + '.zip', os.path.join(log_dir, file))\n",
    "            \n",
    "            os.remove(os.path.join(log_dir, file))\n",
    "              \n",
    "os.remove(temp_csv)\n",
    "\n",
    "print('Completed at: ' + datetime.date.today().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

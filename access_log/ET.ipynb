{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import helper as hp\n",
    "import sq\n",
    "import jp\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "Gb = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_csv = 'v:'+os.sep+'temp'+os.sep+'temp.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = 'v:'+os.sep+'temp'+os.sep+'log'+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_dir = 'v:'+os.sep+'temp'+os.sep+'a'+os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlitedb = 'v:\\\\temp\\\\access_log.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disk_engine = create_engine('sqlite:///'+sqlitedb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store = pd.HDFStore('access_log.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#squid = sq.read_squid_log('access.log')\n",
    "#squid.tail()\n",
    "#safe_websites = 'google|microsoft|trendmicro|gstatic.com|bdpinsight.eu|.gov.hk'\n",
    "#access = squid[squid['URL'].str.contains(safe_websites) == False]\n",
    "#sum(squid.bytes)/Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def valid_file(file):\n",
    "    if (file[-4:] != '.zip' and file[0:9] == 'hk-ssg140'):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2015-10-26 00:00:00\n",
      "2292843 rows processed.\n",
      "hk-ssg140.log-20151023 processed\n",
      "2292320 rows processed.\n",
      "hk-ssg140.log-20151024 processed\n",
      "1333621 rows processed.\n",
      "hk-ssg140.log-20151025 processed\n",
      "Completed at: 2015-10-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print('Started at: ' + time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "mode = 'full'\n",
    "if (mode == 'single'):\n",
    "    yesterday = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    log_file = os.path.join(log_dir, 'hk-ssg140.log-'+yesterday.strftime('%Y%m%d'))\n",
    "                                                                         \n",
    "    jp.clean_juniper_file(log_file, temp_csv)\n",
    "    \n",
    "    juniper = [] \n",
    "    temp_result = []\n",
    "    \n",
    "    juniper = jp.read_syslog_juniper(temp_csv)\n",
    "    \n",
    "    juniper['date'] = juniper['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    juniper['total_size'] = juniper['sent_size'] + juniper['received_size']        \n",
    "    \n",
    "    temp_result = jp.process_group(juniper)\n",
    "    \n",
    "    temp_result = pd.DataFrame(temp_result, columns=['date', 'src', 'dst', 'count', 'total_size'])\n",
    "    temp_result['location'] = 'HK'\n",
    "    \n",
    "    try:\n",
    "        temp_result.to_sql('data', disk_engine, index=False, if_exists='append')\n",
    "        print (log_file + ' processed')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    hp.zip_file(zip_dir + log_file + '.zip', os.path.join(log_dir, log_file))\n",
    "            \n",
    "    os.remove(os.path.join(log_dir, log_file))                                                                         \n",
    "                                                                         \n",
    "else:    \n",
    "    for file in os.listdir(log_dir):\n",
    "        if (valid_file(file)):\n",
    "            log_file = os.path.join(log_dir, file)\n",
    "            jp.clean_juniper_file(log_file, temp_csv)\n",
    "    \n",
    "            juniper = [] \n",
    "            temp_result = []\n",
    "    \n",
    "            juniper = jp.read_syslog_juniper(temp_csv)\n",
    "    \n",
    "            juniper['date'] = juniper['time'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "            juniper['total_size'] = juniper['sent_size'] + juniper['received_size']        \n",
    "    \n",
    "            temp_result = jp.process_group(juniper)\n",
    "    \n",
    "            temp_result = pd.DataFrame(temp_result, columns=['date', 'src', 'dst', 'count', 'total_size'])\n",
    "            temp_result['location'] = 'HK'\n",
    "    \n",
    "            try:\n",
    "                temp_result.to_sql('data', disk_engine, index=False, if_exists='append')\n",
    "                print (file + ' processed')\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            hp.zip_file(zip_dir + file + '.zip', os.path.join(log_dir, file))\n",
    "            \n",
    "            os.remove(os.path.join(log_dir, file))\n",
    "              \n",
    "os.remove(temp_csv)\n",
    "\n",
    "print('Completed at: ' + time.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
